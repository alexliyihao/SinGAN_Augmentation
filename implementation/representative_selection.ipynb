{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I take a random dataset here\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "data /= np.max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the hyperparameter\n",
    "N_represenative = 40\n",
    "PCA_N_components = 5\n",
    "DBSCAN_eps = 0.5\n",
    "DBSCAN_min_samples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=PCA_N_components)\n",
    "X_extracted = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_phase_1 = DBSCAN(eps = DBSCAN_eps, min_samples = DBSCAN_min_samples)\n",
    "clustering_phase_1.fit(X_extracted)\n",
    "cluster_label = clustering_phase_1.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_distribution(N_represenative, label):\n",
    "    \"\"\"\n",
    "    given the cluster label generated by DBSCAN, calculate the number of representative for each cluster\n",
    "    \n",
    "    input: \n",
    "        label: numpy.ndarray, the output of sklearn.cluster.DBSCAN's label_ method\n",
    "    output: \n",
    "        rep_distribution: numpy.ndarray, the number of representative for each cluster, in the same order\n",
    "    \"\"\"\n",
    "    label_, count = np.unique(label, return_counts = True)\n",
    "    #remove the noise term(cluster = -1)\n",
    "    count = count[1:]\n",
    "    rep_distribution = np.ceil(count*N_represenative/count.sum()).astype(\"int\")\n",
    "    return rep_distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 30,  2,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_distribution = get_rep_distribution(N_represenative, cluster_label)\n",
    "rep_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rep_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_cluster_p1(data, label):\n",
    "    \"\"\"\n",
    "    given the original dataset and the label from sklearn.cluster, return the list of each cluster\n",
    "    \n",
    "    input:\n",
    "        data: numpy.ndarray, original data passed into sklearn.cluster.DBSCAN's fit method\n",
    "        label: numpy.ndarray, the output of sklearn.cluster.DBSCAN's label_ method\n",
    "    output:\n",
    "        cluster_p1: list of numpy.ndarray, the feature divided by labels\n",
    "    \"\"\"\n",
    "    unique_label = np.unique(label)[1:]\n",
    "    cluster_p1 = [[] for i in np.arange(unique_label.shape[0])]\n",
    "    for i in np.arange(np.shape(label)[0]):\n",
    "        if (label[i] != -1):\n",
    "            cluster_p1[label[i]].append(data[i])\n",
    "    cluster_p1 = [np.array(i) for i in cluster_p1]\n",
    "    return cluster_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_p1 = get_sub_cluster_p1(data = X_extracted, label = cluster_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(cluster, means):\n",
    "    \"\"\"\n",
    "    find the single sample(image) in the cluster which has smallest euclidean distance to mean\n",
    "    \n",
    "    input:\n",
    "        cluster: numpy.ndarray, a set of sample(image)\n",
    "        mean: numpy.ndarray, an array of means, from the output of sklearn.mixture.GMM.means_\n",
    "        \n",
    "    output:\n",
    "        rep: numpy.ndarray, the sample in the cluster which has smallest euclidean distance to each mean respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    rep_index = np.argmin(euclidean_distances(cluster, means), axis = 0)\n",
    "    reps = cluster[rep_index]\n",
    "    return reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rep_list = []\n",
    "for i in range(len(cluster_p1)):\n",
    "    sub_cluster = cluster_p1[i]\n",
    "    num_rep = int(rep_distribution[i])\n",
    "    GMM = GaussianMixture(n_components = num_rep)\n",
    "    GMM.fit(sub_cluster)\n",
    "    means = GMM.means_\n",
    "    reps = find_closest(cluster = sub_cluster, means = means)\n",
    "    rep_list += list(np.where(X_extracted == rep)[0][1] for rep in reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_selection(data, N_represenative, PCA_N_components, DBSCAN_eps, DBSCAN_min_samples):\n",
    "    \"\"\"\n",
    "    The representative selection procedure\n",
    "    it run 1. PCA to original data \n",
    "           2. DBSCAN for cluster phase 1 \n",
    "           3. Gaussian mixture model on each cluster in phase 1\n",
    "           4. Find the point with smallest euclidean distance to these means\n",
    "    and returns the index of these points in original data\n",
    "    \n",
    "    input:\n",
    "        data: numpy.ndarray, the dataset of image, in current setting, each image need to be flattened (sklearn.PCA only takes flattened value)\n",
    "        N_represenative: int, the minimal value of representative, the final result may have higher number\n",
    "        PCA_N_components: int, the number of component for PCA\n",
    "        DBSCAN_eps: float, the eps hyperparameter fpr DBSCAN\n",
    "        DBSCAN_min_samples: int\n",
    "        \n",
    "    output:\n",
    "        rep_list: list of int, the index of representatives in the dataset\n",
    "                  (which can be use as index of np.array directly)\n",
    "    \"\"\"\n",
    "    #scale all the value to 0-1 scale\n",
    "    data /= np.max(data)\n",
    "    \n",
    "    #run PCA on X\n",
    "    pca = PCA(n_components=PCA_N_components)\n",
    "    X_extracted = pca.fit_transform(data)\n",
    "    #run DBSCAN on the principle components, get the label\n",
    "    clustering_phase_1 = DBSCAN(eps = DBSCAN_eps, min_samples = DBSCAN_min_samples)\n",
    "    clustering_phase_1.fit(X_extracted)\n",
    "    cluster_phase_1_label = clustering_phase_1.labels_\n",
    "\n",
    "    #calculate the distribution based on the cluster p1\n",
    "    rep_distribution = get_rep_distribution(N_represenative = N_represenative, label = cluster_phase_1_label)\n",
    "    \n",
    "    #get the actual sub cluster\n",
    "    cluster_p1 = get_sub_cluster_p1(data = X_extracted, label = cluster_phase_1_label)\n",
    "    \n",
    "    rep_list = []\n",
    "    \n",
    "    #for each cluster\n",
    "    for i in range(len(cluster_p1)):\n",
    "        \n",
    "        sub_cluster = cluster_p1[i]\n",
    "        num_rep = rep_distribution[i]\n",
    "        \n",
    "        #run a GMM to find the mean\n",
    "        GMM = GaussianMixture(n_components = num_rep)\n",
    "        GMM.fit(sub_cluster)\n",
    "        means = GMM.means_\n",
    "        \n",
    "        #find the closest image\n",
    "        reps = find_closest(cluster = sub_cluster, means = means)\n",
    "        #add them to the closest image\n",
    "        rep_list += list(np.where(X_extracted == rep)[0][1] for rep in reps)\n",
    "        \n",
    "    return rep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_list = representative_selection(data = digits.data,\n",
    "                                    N_represenative = 40, \n",
    "                                    PCA_N_components = 5, \n",
    "                                    DBSCAN_eps = 0.5, \n",
    "                                    DBSCAN_min_samples = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "digits.data[rep_list].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
